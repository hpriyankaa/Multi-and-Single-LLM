# A Multi-Stage LLM Pipeline for Reliable Python Syntax Error Explanations

## Overview

This repository contains the implementation and evaluation of two approaches for generating explanations and fixes for Python syntax errors using Large Language Models (LLMs):

1. A **single-LLM baseline**, where one model directly produces an explanation and fix.
2. A **multi-stage, multi-LLM pipeline**, where multiple models propose fixes, critique each other, and a judge model synthesizes a final output.

The objective of this work is not to demonstrate that single LLMs are incapable of producing correct feedback, but to systematically evaluate whether **structured verification and synthesis across multiple models improves reliability, consistency, and explainability** of generated feedback.

---

## Dataset

The experiments are conducted on a dataset of **100 Python syntax-error questions**, where each instance contains:

- Buggy Python source code
- Execution feedback (error message / traceback)
- Question metadata (QID, description)

The same dataset is used across both pipelines to ensure fair comparison.

---

## Single-LLM Baseline Pipeline

### Description

In the single-LLM baseline, each question is processed independently by a single model. The model is prompted with the buggy code and execution feedback and is expected to generate:

- A natural language explanation of the error
- A fix outline describing the corrective steps
- A confidence score

The output is stored in a structured JSON format and exported to an Excel sheet for analysis.

### Purpose

This pipeline serves as a **control baseline**, representing the standard “prompt → response” approach commonly used in prior work and practice.

---

## Multi-Stage Multi-LLM Pipeline

The core contribution of this work is a **three-stage verification pipeline** that introduces diversity, critique, and synthesis into the debugging process.

---

### Stage A: Independent Patch Proposals

In Stage A, multiple LLMs (e.g., Qwen and GPT-OSS) are queried independently using the same routed input.

Each model produces a structured JSON output containing:

- A diagnosis of the syntax error
- A minimal fix outline (list of steps)
- A confidence score

Models do not see each other’s outputs, ensuring independent reasoning paths.  
The results of this stage are stored in `stageA_proposals.xlsx`.

---

### Stage B: Cross-Model Critique (Debate Phase)

In Stage B, models critique the proposals generated by other models.

Each critique focuses on concrete, evidence-grounded issues such as:

- Whether the proposed fix addresses the exact syntax error
- Missing edge cases or incorrect assumptions
- Unnecessary or unsafe code changes

Critiques are structured and stored as JSON objects listing identified issues.  
This stage converts implicit disagreement into **explicit, inspectable artifacts** and is stored in `stageB_critiques.xlsx`.

---

### Stage C: Judge Synthesis

In Stage C, a judge model receives:

- The original buggy code and execution feedback
- All Stage A proposals
- All Stage B critiques

Instead of performing majority voting, the judge:

- Selects the proposal that best survives critique, or
- Produces a **hybrid fix** combining strengths from multiple proposals

The final output includes:

- A single consolidated explanation
- A final fix outline
- The chosen source (Qwen, GPT-OSS, or Hybrid)
- A confidence score

The results are stored in `stageC_final_helpful_output.xlsx`.

---

## Output Representation

All outputs across stages are constrained to **strict JSON schemas**, enabling:

- Automatic parsing
- Metric computation
- Auditable comparison across models and stages

Key structured fields include:

- `final_helpful_output`
- `final_fix_outline`
- `confidence`
- `chosen_source`

Malformed or missing outputs are explicitly tracked as parse failures.

---


## Relationship to Prior Work

Prior work such as *PyFiXV* focuses on validating whether generated feedback should be shared with students.  
This work extends that idea by introducing **multi-agent critique and synthesis**, where feedback is not merely accepted or rejected, but actively refined through structured disagreement.

This positions the pipeline as a **multi-agent verification pattern**, rather than a simple ensemble or voting-based system.

---

## Summary

This work demonstrates that while single LLMs often produce correct feedback, introducing structured multi-stage verification leads to:

- More consistent explanations
- Better alignment with execution evidence
- Improved transparency and auditability

The proposed pipeline provides a practical and extensible framework for reliable LLM-assisted programming feedback.
